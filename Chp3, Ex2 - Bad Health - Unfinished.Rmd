# Chapter 3, Exercise 2

Find two datasets from the UCI data repository or R datasets. Conduct a detailed analysis for both datasets using both logistic regression model and the tree model, e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

********************************************************************

Issues: There were no issues with the logistic regression, but the decision tree comes out to only be one node. I was going to get another dataset. 

********************************************************************

## Bad Health Dataset 

We will analyze the German health survey dataset with 1,127 observations and 3 variables. In particular, we will attempt to predict if a person claims to be in bad health or in good health based on the number of doctor visits and the patient's age in 1998. 
```{r}
library(RCurl)

# load the dataset
BH <- read.csv(text=getURL("https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/COUNT/badhealth.csv"))

# remove the identifer column 
BH <- BH[, -1]
```

| Feature Name | Description                                                |
|--------------|------------------------------------------------------------|
| numvisit     | number of visits to doctor during 1998                     |
| badh         | 1=patient claims to be in bad health; 0=not in bad health  |
| age          | age of patient: 20-60                                      |

We will create a model based solely on the person's age. We notice that age is an important predictor and this model explains 729.66 - 687.69 = 41.97 of the total deviance. 

### Logistic Regression

```{r}
logit.BH1 <- glm(badh ~ age, data = BH, family = "binomial") 
summary(logit.BH1)
```

We will create a model based solely on the person's number of visits to the doctor during 1998. We notice that numvisit is an important predictor and this model explains 729.66 - 632.00 = 97.66 of the total deviance.
```{r}
logit.BH2 <- glm(badh ~ numvisit, data = BH, family = "binomial") 
summary(logit.BH2)
```

We will create a model based on all of the possible predictors. We notice that age and numvisit are important predictors and they have reduced the total deviance by 729.66 - 603.43 = 126.23 with a loss of 1 degree of freedom. 

We have now created and compared all of the possible logistic regressions for this dataset and have concluded that the following one is the best.
```{r}
logit.BH3 <- glm(badh ~ age + numvisit, data = BH, family = "binomial") 
summary(logit.BH3)
```

We calculate the 95% confidence intervals of the regression parameters using profiled log-likelihood. We notice that the CI for age is very narrow, which means that it is a precise estimate of the age. 
```{r} 
confint(logit.BH3)
```

We derive the odds ratio and their 95% confidence intervals. 
```{r} 
exp(cbind(OR = coef(logit.BH3), confint(logit.BH3)))
```

### Decision Trees
We will plot the decision tree using the function rpart().

```{r} 
# set the theme of the tree
theme_set(theme_gray(base_size = 15) ) 

# Get the column number of the outcome variable
target_indx <- which( colnames(BH) == "numvisit" ) 

# Change the outcome variables from 0 and 1 to c0 and c1
BH[,target_indx] <- as.factor(paste0("c", BH[,target_indx]))

# create the tree
tree <- rpart( numvisit ~ badh + age, BH) 
prp(tree, nn.cex=1)
```


.
.
.
.



Our model can be applied to determine whether or not a patient believes that they are in bad health. A patient who believes they are in bad health may choose to visit the doctor more or may be more open to take presciption drugs. On the other hand, a person who believes they are in good health might be less motivated to take medication or follow up on recommended doctor's visits. 







