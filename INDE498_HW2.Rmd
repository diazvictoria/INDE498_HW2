---
title: "INDE498_HW2"
author: "Steven Hwang, 	Haena Kim, Victoria Diaz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)

```
# Chapter 2, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.

```{r}
#data set from Chapter 2, Exercise 1
library(RCurl)
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD2.csv"))
AD$ID = c(1:dim(AD)[1])
str(AD)
```

```{r}
# try full-scale model - exclude MMSCORE as it is other output, trying to predict TOTAL13
data <- AD[,c(1:18)]
data <- subset(data, select = -c(MMSCORE) )
names(data)

data$TOTAL13<- floor(data$TOTAL13)

#splitting the data by half
sample_first_half <- sample(nrow(data),floor( nrow(data)/2) )

check<-data[sample_first_half,]
check_2<-data[-sample_first_half,]
#because the nrow(check) = 258 and nrow(check_2) = 259, take one row out from check_2 to make both dataset in the same length
check_2<-check_2[1:(nrow(check_2)-1),]


#tree model - no model selection
tree <- rpart( TOTAL13 ~ ., data = check) 
prp(tree, nn.cex = 1)

#regression model - no model selection
lm.AD <- lm(TOTAL13 ~ ., data = check)
summary(lm.AD)

```

```{r}

#prediction - tree
tree_pred_with_second_half<-floor(predict(tree, check_2))

current_error_train <- length(which(tree_pred_with_second_half != check$TOTAL13))/length(tree_pred_with_second_half)

MSE_tree<-mean((check$TOTAL13-tree_pred_with_second_half)^2)
print(paste("MSE_tree is ",MSE_tree))

#prediction - regression
regression_pred_with_second_half<-floor(predict(lm.AD, check_2))
MSE_re<-mean((check$TOTAL13-regression_pred_with_second_half)^2)
print(paste("MSE_re is ",MSE_re))
```

Mean square error for regression is smaller than tree model. Therefore, we chose regression model over the tree model. We tried to farther improve the model in the next following sections.

Improvement for regression model
```{r}
# model selection
lm.AD.F <- step(lm.AD, direction="backward", test="F")
summary(lm.AD.F)
anova(lm.AD.F ,lm.AD)
```


Improvement for Tree
```{r,cache=FALSE}
library(dplyr)
set.seed(1)
errintrain <- NULL
errintest <- NULL
leaf.v <- NULL
cp<- NULL

for(i in seq(0.2,0,by=-0.005) ){
  tree <- rpart( TOTAL13 ~ ., data = check, cp= i  ) 
  pred.train <- floor(predict(tree, check))
  pred.test <- floor(predict(tree, check_2))
  current_error_train <- length(which(pred.train != check$TOTAL13))/length(pred.train)
  current_error_test <- length(which(pred.test != check_2$TOTAL13))/length(pred.test)
  errintrain <- c(errintrain, current_error_train)
  errintest <- c(errintest, current_error_test)
  leaf.v <- c(leaf.v, length(which(tree$frame$var == "<leaf>")))
  cp <- c(cp,i)
}
err.mat <- as.data.frame( cbind( train_err = errintrain, test_err = errintest , leaf_num = leaf.v ,cp_table = cp) )
err.mat$leaf_num <- as.factor( err.mat$leaf_num  )
err.mat <- unique(err.mat)
err.mat <- err.mat %>% gather(type, error, train_err,test_err)
print(err.mat)
```


The test errors and train errors according to different size of the leaf nodes are plotted.  Train errors generatlly decrease while the test errerors first decrease and then increase at leaf number eqauals to 4,5,6. As the gap between test and train error data is the smallest at leaf number equal to 4, the adequate number of leaf node would be 4. Other leaf numbers may result overfitting of predicted data more.
```{r}
data.plot <- err.mat %>% mutate(type = type)
ggplot(data.plot, aes(x=leaf_num, y=error, shape = type, color=type)) + geom_line() +
  geom_point(size=5) 

```

Final decision tree model can be selected with the 4 decision (leaf) nodes WITH cp = 0.05

```{r}
tree_0.05 <- prune(tree, cp = 0.05)
prp(tree_0.05, nn.cex = 1)

```



```{r}
#MSE for improved tree
tree_pred_with_second_half_improved<-floor(predict(tree_0.05, check_2))

current_error_train <- length(which(tree_pred_with_second_half_improved != check$TOTAL13))/length(tree_pred_with_second_half_improved)

MSE_tree_improved<-mean((check$TOTAL13-tree_pred_with_second_half_improved)^2)
print(paste("MSE_tree is ",MSE_tree))
print(paste("MSE_tree_improved is ",MSE_tree_improved))

#MSE for improved regression model
regression_pred_with_second_half_improved<-floor(predict(lm.AD.F, check_2))
MSE_re_improved<-mean((check$TOTAL13-regression_pred_with_second_half_improved)^2)
print(paste("MSE_re is ",MSE_re))
print(paste("MSE_re_improved is ",MSE_re_improved))
```

After improving the both regression and tree models, mean square error for each model decreased, indicating the improvement of the model predictions (for tree, MSE went down from 112.4 to 103.69 and for regression MSE went down from 102.94 to 100.68. For both original and improved model, we chose regression model over tree model because it had lower MSE. 

Improvements in MSE between original tree model and regression model was bigger than those in MSE for improved model. The MSE differences for the original models was 9.46 (112.40 (tree)- 102.94 (regression) = 9.46). The MSE differences in improved models went down from 9.46 to 3.01 (103.69 (tree)- 100.68 (regression) = 3.01). This indicates that the model selection between improved tree and regression models became less distinctive than those between original models.

# Chapter 2, Exercise 4 
Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). Thus, it is not uncommon to assign a weight to each data point. Denote the weight for the ith data point as w_i. We still want to estimate the regression parameters in the least squares framework. Follow the process of the derivation of the least squares estimator and propose your new estimator of the regression parameters.

The weighted mean square error in matrix is:
\[ min(WMSE(b)) = \frac{1}{n} \sum_{i=1}^{n} w_{i}(y_i - x_ib)^2 \]

We can rewrite $w_i$ in matrix form as $W$, which is a diaganol matrix where the $i_{th}$ diaganol element is the weight for the $x_{i}$ observation. In matrix form this is:
\[  min(WMSE(b)) = \frac{1}{n} (Y-X\beta)^TW(Y-X\beta)\]
Expanding the terms:
\[  min(WMSE(b)) = \frac{1}{n} (Y^TWY-Y^TWX\beta-\beta^TX^TWY+\beta^TX^TWX\beta)\]
Differentiating with respect to $\beta$ and setting equal to zero:
\[  min(WMSE(b)) = \frac{2}{n} (-X^TWY+X^TWX\beta)\]
Setting this equal to 0, we get:
\[  \hat{\beta} = (X^TWX)^{-1}(X^TWY) \]



# Chapter 3, Exercise 1
Create a new binary variable based on AGE, by labeling the subjects whose age is above the mean of AGE to be class “1” and labeling the subjects whose age is below the mean of AGE to be class “0”. Then, repeat the analysis shown in the R lab of this chapter for the logistic regression model and the analysis shown in the R lab of Chapter 2 for decision tree model. Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

We will use all of the predictors (except for AGE, MMSCORE, TOTAL13, and DX_bl) to predict where a person's age is above or below the mean age. 

## Logistic Regression Model
We begin by loading the data and creating new column named AGE_bin.
AGE_bin: Contains "1" if the subject's age is >= mean(AGE); Contains "0" if the subject's age is < mean(AGE).
```{r}
library(RCurl)
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD.csv"))
AD$ID = c(1:dim(AD)[1])
AD$AGE_bin = ifelse(AD$AGE >= mean(AD$AGE), 1, 0)
AD = AD[ , !(names(AD) %in% c("AGE", "MMSCORE", "TOTAL13", "DX_bl"))]

```

Fitting our model using all of the predictors yields the significant predicators as HippoNV, e4_1, and PTEDUCAT.
```{r}
logit.AD.1 <- glm(AGE_bin ~ ., data = AD[, -c(15)], family = "binomial") 
summary(logit.AD.1)
```

Fitting the model based on the significant predicators of the last model gives us that only two out of the three predictors (HippoNV and e4_1) are actually significant. 
```{r}
logit.AD.2 <- glm(AGE_bin ~ HippoNV + e4_1 + PTEDUCAT , data = AD[, -c(15)], family = "binomial") 
summary(logit.AD.2)
```

We will use the following visualization of the relationships between some of the predicators and the outcome in order to make an educated guess on which other predictors should be considered. 

None of the ploted variables seem to be able to properly classify the data (since all of the pairs of box overlap significantly). 
```{r}
require(reshape2)

AD.long <- melt(AD[,c(2:5, 15, 16)], id.vars = c("ID", "AGE_bin"))

# Plot the data using ggplot
require(ggplot2)
p <- ggplot(AD.long, aes(x = factor(AGE_bin), y = value))

# boxplot, size=.75 to stand out behind CI
p <- p + geom_boxplot(size = 0.75, alpha = 0.5)

# points for observed data
p <- p + geom_point(position = position_jitter(w = 0.05, h = 0), alpha = 0.1)

# diamond at mean for each group
p <- p + stat_summary(fun.y = mean, geom = "point", shape = 18, size = 6,
                      alpha = 0.75, colour = "red")

# confidence limits based on normal distribution
p <- p + stat_summary(fun.data = "mean_cl_normal", geom = "errorbar",
                      width = .2, alpha = 0.8)

p <- p + facet_wrap( ~ variable, scales = "free_y", ncol = 3)


p <- p + labs(title = "Boxplots of variables by mean(age) (0: < mean;1: >= mean)")

print(p)
```

We will use the step() function to automatically choose the best model. The significant variables are  HippoNV, e4_1, and PTEDUCAT. This model explains all but 75.13 of the total deviance with 4 less degrees of freedom. 
```{r} 
logit.AD.full <- glm(AGE_bin ~ ., data = AD[!(names(AD) %in% c("ID"))], family = "binomial")
logit.AD.final <- step(logit.AD.full, direction="both", trace = 0)
summary(logit.AD.final)
```

We can find the 95% confidence intervals of the regression parameters.
```{r}
## CISs of the regression parameters using profiled log-likelihood 
confint(logit.AD.final) 
```

We can also use the Wald Test to test the significance of the regression parameters.
```{r} 
library(aod) 
 
wald.test(b = coef(logit.AD.final), Sigma = vcov(logit.AD.final), Terms = 2)
```

We will now test how our model works on data. We will randomnly choose 200 samples from the AD dataset to make AD.pred. We will create a 95% CI for these predications.
```{r} 
# Dataset that we will test our prediction model on 
AD.pred <- AD[sample(1:dim(AD)[1], 200),]

# pred will have our predictions
pred <- predict(logit.AD.final, AD.pred, type = "link", se.fit = TRUE) 
AD.pred$fit <- pred$fit 
AD.pred$se.fit <- pred$se.fit

# CI for fitted values
AD.pred <- within(AD.pred, {
  # add "fitted" to make predictions at appended temp values 
  fitted = exp(fit)/(1 + exp(fit))
  fit.lower = exp(fit - 1.96 * se.fit) / (1 + exp(fit - 1.96 * se.fit))
  fit.upper = exp(fit + 1.96 *se.fit) / (1 + exp(fit + 1.96 * se.fit))
})


# Visualizing the predication 
library(ggplot2)
newData <- AD.pred[order(AD.pred$AGE_bin),]
p <- ggplot(newData, aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, y = AGE_bin))
# predicted curve and point-wise 95% CI
p <- p + geom_ribbon(aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, ymin = fit.lower, ymax = fit.upper), alpha = 0.2)
p <- p + geom_line(aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, y = fitted), colour="red")
# fitted values
p <- p + geom_point(aes(y = fitted), size=2, colour="red")
# observed values
p <- p + geom_point(size = 2)
p <- p + ylab("Probability")
p <- p + labs(title = "Observed and predicted probability of Predicting Age")
print(p)
```

We will test whether or not there is a lack-of-fit. Since dev.p.val is 8.162904e-05, which is not greater than 0.10, there is a large lack of model fit. We conclude that the error in our predictions are coming from a lack of fit from the model. 
```{r}
# Test residual deviance for lack-of-fit (if > 0.10, little-to-no lack-of-fit)
dev.p.val <- 1 - pchisq(logit.AD.final$deviance, logit.AD.final$df.residual)
dev.p.val
```

We will conclude by computing the odds ratios for our predicators and their corresponding 95% confidence intervals to determine the influence of the predictors. 
```{r}
## odds ratios and 95% CI
exp(cbind(OR = coef(logit.AD.final), confint(logit.AD.final)))
```

## Decision Tree
We now create a decision tree based on the dataset. We see that the splitting happens with regards to the predictors HippoNV, FDG, rs3865444, and rs610932. 
```{r}
AD$AGE_bin <- as.factor(AD$AGE_bin)
AD.tree <- rpart(AGE_bin ~., data = AD[!(names(AD) %in% c("ID"))])
prp(AD.tree, varlen=5)
```

When we look at the variable importance of each predictor, we see that the most important variables are HippoNV and FDG. 
```{r}
print(AD.tree$variable.importance)
```


Our objective is now to prune the tree. Testing different different values for cp, we see that our decision tree is most accurate when our tree has about 3 to 4 leaves. 
```{r,cache=FALSE}
library(magrittr)
library(tidyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(dplyr)
library(partykit)

set.seed(1)
train.ix <- sample(nrow(AD),floor( nrow(AD)/2) )
err.train.v <- NULL
err.test.v <- NULL
leaf.v <- NULL
for(i in seq(0.2,0,by=-0.005) ){
  tree <- rpart( AGE_bin ~ ., data = AD[train.ix,], cp=i  ) 
  pred.train <- predict(tree, AD[train.ix,],type="class")
  pred.test <- predict(tree, AD[-train.ix,],type="class")
  current.err.train <- length(which(pred.train != AD[train.ix,]$AGE_bin))/length(pred.train)
  current.err.test <- length(which(pred.test != AD[-train.ix,]$AGE_bin))/length(pred.test)
  err.train.v <- c(err.train.v, current.err.train)
  err.test.v <- c(err.test.v, current.err.test)
  leaf.v <- c(leaf.v, length(which(tree$frame$var == "<leaf>")))
}
err.mat <- as.data.frame( cbind( train_err = err.train.v, test_err = err.test.v , leaf_num = leaf.v) )
err.mat$leaf_num <- as.factor( err.mat$leaf_num  )
err.mat <- unique(err.mat)
err.mat <- err.mat %>% gather(type, error, train_err,test_err)

# visualizing this 
data.plot <- err.mat %>% mutate(type = factor(type))
ggplot(data.plot, aes(x=leaf_num, y=error, shape = type, color=type)) + geom_line() +
  geom_point(size=3) 
```

We plot a more optimal decision tree which is only dependent on the two most important variables. 
```{r}
tree_0.05 <- prune(tree,cp =0.0319, depth = 3)
prp(tree_0.05,nn.cex=1)
```

Both the logistic regression and the decision tree showed the importance of HippoNV as a predictor. The logistic model showed that PTGENDER, PTEDUCAT, and e4_1 are important predictors, while the decision tree showed that FDG is an important predictor. 


# Chapter 3, Exercise 2
Find two datasets from the UCI data repository or R datasets. Conduct a detailed analysis for both datasets using both logistic regression model and the tree model, e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Medical School Admission
The first dataset we chose to analyze was the MedGPA dataset from the Stat2Data package. This dataset contains data about medical school admission status and information on GPA and standardized test scores. A table that provides a description of the variables included in the data set is provided below.

| Variable Name| Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Accept       | Status: A=accepted to medical school or D=denied admission                                  |
| Acceptance   | Indicator for Accept: 1=accepted or 0=denied                                                |
| Sex          | F=female or M=male                                                                          |
| BCPM         | fuel consumption miles per US gallon                                                        |
| GPA          | College grade point average                                 |
| VR           | Verbal reasoning (subscore                                 |
| PS           | Physical sciences (subscore)                                  |
| WS           | Writing sample (subcore)                                 |
| BS           | Biological sciences (subscore)                                  |
| MCAT         | Score on the MCAT exam (sum of CR+PS+WS+BS)                                 |
| Apps         | Number of medical schools applied to                                 |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, sex, GPA, PS, WS, and BS to be significant. Looking at the summary we can see that comparing males to females, males have a 2.84 increase in log odds of acceptance versus females. We also found that GPA, PS, and BS all have a negative log odds of admission for each unit of increase. Only WS had a positive log odds of admission for each unit of increase. This model can be used to assess a candidates probability of being accepted into medical school and can be used to give insight into what variables best increase their chance of acceptance. 

```{r}
df.ch3ex2.med <- read.csv("MedGPA.csv")
ch3ex2.med.log <- glm( Accept~.,family=binomial(link='logit'),data=df.ch3ex2.med[,-c(1,3)])
ch3ex2.med.log <- step(ch3ex2.med.log, direction = "backward", trace = 1)
summary(ch3ex2.med.log)
```
Next a decision tree was fit on the same dataset. The tree only found BS to be the variable to split on. The tree does not have as much application in this context due to the limited data set size. If the data set size was larger, the tree would allow a participant to find the best values for each of the predictor variables that would best increase their chance of being accepted into medical school. 

```{r}
ch3ex2.med.tree <- rpart(Accept~., data=df.ch3ex2.med[,-c(1,3)], control = rpart.control(p = 0.0001))
prp(ch3ex2.med.tree, varlen=3)
```

## Bad Health
The second dataset we chose to analyze was the BadHealth dataset from the COUNT package. This dataset contains data about a German health survey data for the year 1998. A table that provides a description of the variables included in the data set is provided below.

| Variable Name           | Description                                                                                 |
|--------------           |---------------------------------------------------------------------------------------------|
| Number of visits        | Number of visits to doctor during 1998                                 |
| bad health              | 1=patient claims to be in bad health; 0=not in bad health                                               |
| age                     | age of patient: 20-60                                                                          |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, number of visits, and age to be significant. Looking at the summary we can see that for every unit increase in the number of visits to the doctor, there is an increase of 0.22 log odds of the patient claiming to be in bad health. For every unit increase in the age of the patient, there is an increase of 0.05 log odds of the patient claiming to be in bad health. This type of model can be used to assess the health of patients using easily accessible data and can be used in policy making.

```{r}
df.ch3ex2.bh <- read.csv("badhealth.csv")
df.ch3ex2.bh$badh <- as.factor(df.ch3ex2.bh$badh)
ch3ex2.med.bh <- glm( badh~.,family=binomial(link='logit'),data=df.ch3ex2.bh[,-c(1)])
ch3ex2.med.bh <- step(ch3ex2.med.bh, direction = "backward", trace = 1)
summary(ch3ex2.med.bh)
```
Next a decision tree was fit on the same dataset. The tree split on number of visits and on age. The tree found that if the number of visits is greater than 14 and the patients age was greater than 36, the patient most likely said they were in bad health. If the number of visits was less than 7.5 or the patient was less than the age of 36, the patient most likely reported they were not in bad health. This type of model can to identify an easy rule of assessing the overall health of a population using the number of visits to the doctor, which would be useful in policy making.

```{r}
ch3ex2.bh.tree <- rpart(badh~., data=df.ch3ex2.bh[,-c(1)], control = rpart.control(p = 0.0001))
prp(ch3ex2.bh.tree, varlen=3)
```

# Chapter 3, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.

```{r}
df.ch3ex2.bh <- read.csv("badhealth.csv")
# df.ch3ex2.bh$badh <- as.factor(df.ch3ex2.bh$badh)

#divide dataset into two
data <- df.ch3ex2.bh[,-c(1)]
sample_first_half <- sample(nrow(data),floor( nrow(data)/2) )

check<-data[sample_first_half,]
```
  
logistic regression model

```{r}
check$badh <- as.factor(check$badh)

ch3ex3.bh_logit <- glm( badh~.,family=binomial(link='logit'),data=check[,-c(4)])
summary(ch3ex3.bh_logit)
```
numvisit and age are significant as their p-value is less than 0.05. Out of total devience of 346.77, 346.77-294.17 = 52.6 could be explained by the predictor numvisit and age. 

```{r}
confint(ch3ex3.bh_logit)
```

```{r}
library(aod)

wald.test(b = coef(ch3ex3.bh_logit),Sigma=vcov(ch3ex3.bh_logit), Terms=2)
```

```{r}

check_2<-data[-sample_first_half,]
#because the nrow(check) = 563 and nrow(check_2) = 564, take one row out from check_2 to make both dataset in the same length
check_2<-check_2[1:(nrow(check_2)-1),]

# To predict on a given dataset
colnames(check_2) <- paste("",colnames(check_2),sep="")

# predict() uses all the temp values in dataset, including appended values
pred <- predict(ch3ex3.bh_logit, check_2, type = "link", se.fit = TRUE)
check_2$fit <- pred$fit
check_2$se.fit <- pred$se.fit
```


```{r}
# CI for fitted values
check_2 <- within(check_2, {
 # added "fitted" to make predictions at appended temp values
 fitted = exp(fit) / (1 + exp(fit))
 fit.lower = exp(fit - 1.96 * se.fit) / (1 + exp(fit - 1.96 * se.fit))
 fit.upper = exp(fit + 1.96 * se.fit) / (1 + exp(fit + 1.96 * se.fit))
})
```


relationship with numvisit and badh
```{r}
# visualize the prediction
library(ggplot2)


newData <- check_2[order(check_2$numvisit),]
p <- ggplot(newData, aes(x = numvisit, y = badh))
print(p)
# predicted curve and point-wise 95% CI
p <- p + geom_ribbon(aes(x = numvisit, ymin = fit.lower, ymax = fit.upper), alpha = 0.2)
p <- p + geom_line(aes(x = numvisit, y = fitted), colour="red")
# fitted values
p <- p + geom_point(aes(x = numvisit,y = fitted), size=2, colour="red")
# observed values
p <- p + geom_point(size = 2)
p <- p + ylab("Probability")
p <- p + labs(title = "Observed and predicted probability of numvisit")
print(p)
```
As the graph(relationship between numvisit and badh) shows a logit curve and the prediction confidences are fairly small as the graph shows tight 95% CIs.

Trying other variables
```{r}
# install.packages("reshape2")
require(reshape2)
require(ggplot2)
check$ID = c(1:dim(check)[1])
AD.long <- melt(check[,], id.vars = c("ID", "badh"))

# Plot the data using ggplot
require(ggplot2)
p <- ggplot(AD.long, aes(x = factor(badh), y = value))
# boxplot, size=.75 to stand out behind CI
p <- p + geom_boxplot(size = 0.75, alpha = 0.5)
# points for observed data
p <- p + geom_point(position = position_jitter(w = 0.05, h = 0),
alpha = 0.1)
# diamond at mean for each group
p <- p + stat_summary(fun.y = mean, geom = "point", shape = 18, size = 6,
 alpha = 0.75, colour = "red")

# confidence limits based on normal distribution
p <- p + stat_summary(fun.data = "mean_cl_normal", geom = "errorbar",
 width = .2, alpha = 0.8)
p <- p + facet_wrap( ~ variable, scales = "free_y", ncol = 3)
p <- p + labs(title = "Boxplots of variables by diagnosis")
print(p)
```
Both predictors numvisit and age seem to be able to classify two classes significantly. 


Improvement for regression model
```{r}
ch3ex3.bh_logit_improved <- step(ch3ex3.bh_logit, direction = "backward", trace = 1)
summary(ch3ex3.bh_logit_improved)
```
Chi-square test for original logit regression model

```{r}
# Test residual deviance for lack-of-fit (if > 0.10, little-to-no lack-of-fit)
dev.p.val <- 1 - pchisq(ch3ex3.bh_logit$deviance, ch3ex3.bh_logit$df.residual)

dev.p.val
```

Chi-square test for improved logit regression model
```{r}
# Test residual deviance for lack-of-fit (if > 0.10, little-to-no lack-of-fit)
dev.p.val_i <- 1 - pchisq(ch3ex3.bh_logit_improved$deviance, ch3ex3.bh_logit_improved$df.residual)

dev.p.val_i
```

Both models show no lack of fit as the p-value is 1.

```{r}
# coefficients and 95% CI
cbind(coef = coef(ch3ex3.bh_logit), confint(ch3ex3.bh_logit))

cbind(coef = coef(ch3ex3.bh_logit_improved), confint(ch3ex3.bh_logit_improved))


```

```{r}
## odds ratios and 95% CI
exp(cbind(OR = coef(ch3ex3.bh_logit), confint(ch3ex3.bh_logit)))

exp(cbind(OR = coef(ch3ex3.bh_logit_improved), confint(ch3ex3.bh_logit_improved)))
```

```{r}
# evaluate how well the model fits the data
# predicted probabilities
Yhat <- fitted(ch3ex3.bh_logit)
# the observed events 
YObs <- as.numeric(check_2$badh)
# calculate the correlation between the predicted and observed
cor(Yhat,YObs)
# visualize the correlation
tempData = cbind(Yhat,YObs)
require(ggplot2)
qplot(factor(YObs), Yhat, data = check_2, 
      geom=c("boxplot"), fill = factor(check_2$badh),title="Prediction versus Observed")
```

```{r}
# evaluate how well the model fits the data
# predicted probabilities
Yhat <- fitted(ch3ex3.bh_logit_improved)
# the observed events 
YObs <- as.numeric(check_2$badh)
# calculate the correlation between the predicted and observed
cor(Yhat,YObs)
# visualize the correlation
tempData = cbind(Yhat,YObs)
require(ggplot2)
qplot(factor(YObs), Yhat, data = check_2, 
      geom=c("boxplot"), fill = factor(check_2$badh),title="Prediction versus Observed")
```

The result shows that the the model can not seperate the two classes significantly. 

```{r}
#Finding accuracy of the model
library(ROCR)
pred<- check_2$fitted
head(pred)
check_2$badh
hist(pred)

pred <- prediction(pred, check_2$badh)
eval <- performance(pred, "acc")
plot(eval)
max <- which.max(slot(eval,"y.values")[[1]])
acc <- slot(eval,"y.values")[[1]][max]
cut <- slot(eval,"x.values")[[1]][max]

print(c(Accuracy = acc, Cuttoffvalue=cut))

#Reciever Operating Chatasteristic(ROC) Curve

roc <- performance(pred, "tpr","fpr")
plot(roc)
abline(0,1)

#Area Under Curve(AUC)
auc <- performance(pred,"auc")
auc <- unlist(slot(auc,"y.values"))
auc
legend(.6,.3,round(auc,digits=3),title="AUC")

for (i in x){
if (check_2[i,]$se.fit >= cut){
  check_2[i,"pred"]  <- 1
}else{
  check_2[i,"pred"]  <- 0
}
}

```


Our step function didn't improve the 

Tree model
```{r}
check_2$badh <- as.factor(check_2$badh)
ch3ex3.bh.tree <- rpart(badh~., data=check_2)
prp(ch3ex3.bh.tree, varlen=3)
```


```{r}
#prediction - tree
tree_pred_with_second_half<-predict(ch3ex3.bh.tree , check_2,type="class")
t <- table (predictions = tree_pred_with_second_half, actual = check_2$badh)
t

#accuracy matric
sum(diag(t))/sum(t)

library(ROCR)
library(pROC)
tree_pred_with_second_half<-predict(ch3ex3.bh.tree,newdata = check_2,type ='prob')
auc <- auc(check_2$badh,tree_pred_with_second_half[,2])
plot(roc(check_2$badh,tree_pred_with_second_half[,2]))
legend(.6,.3,round(auc,digits=3),title="AUC")
```
Since Area under the curve for logit regression model is bigger, we chose logit regression model. 