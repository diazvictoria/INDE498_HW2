---
title: "INDE498_HW2"
author: "Steven Hwang, 	Haena Kim, Victoria Diaz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)

```
# Chapter 2, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.
  
# Chapter 2, Exercise 4 
Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). Thus, it is not uncommon to assign a weight to each data point. Denote the weight for the ith data point as w_i. We still want to estimate the regression parameters in the least squares framework. Follow the process of the derivation of the least squares estimator and propose your new estimator of the regression parameters.

The weighted mean squeare error in matrix is:
\[ min(WMSE(b)) = \frac{1}{n} \sum_{i=1}^{n} w_{i}(y_i - x_ib)^2 \]

We can rewrite $w_i$ in matrix form as $W$, which is a diaganol matrix where the $i_{th}$ diaganol element is the weight for the $x_{i}$ observation. In matrix form this is:
\[  min(WMSE(b)) = \frac{1}{n} (Y-X\beta)^TW(Y-X\beta)\]
Expanding the terms:
\[  min(WMSE(b)) = \frac{1}{n} (Y^TWY-Y^TWX\beta-\beta^TX^TWY+X^T\beta^TWX\beta)\]
Differentiating with respect to $\beta$ and setting equal to zero:
\[  min(WMSE(b)) = \frac{2}{n} (-X^TWY+X^TWX\beta)\]
Setting this equal to 0, we get:
\[  \hat\beta = (X^TWX)^{-1}(X^TWY) \]



# Chapter 3, Exercise 1
Create a new binary variable based on AGE, by labeling the subjects whose age is above the mean of AGE to be class “1” and labeling the subjects whose age is below the mean of AGE to be class “0”. Then, repeat the analysis shown in the R lab of this chapter for the logistic regression model and the analysis shown in the R lab of Chapter 2 for decision tree model. Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

DONT USE MMSCORE, AGE, TOTAL13, DX_BL

# Chapter 3, Exercise 2
Find two datasets from the UCI data repository or R datasets. Conduct a detailed analysis for both datasets using both logistic regression model and the tree model, e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Medical School Admission
The first dataset we chose to analyze was the MedGPA dataset from the Stat2Data package. This dataset contains data about medical school admission status and information on GPA and standardized test scores. A table that provides a description of the variables included in the data set is provided below.

| Variable Name| Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Accept       | Status: A=accepted to medical school or D=denied admission                                  |
| Acceptance   | Indicator for Accept: 1=accepted or 0=denied                                                |
| Sex          | F=female or M=male                                                                          |
| BCPM         | fuel consumption miles per US gallon                                                        |
| GPA          | College grade point average                                 |
| VR           | Verbal reasoning (subscore                                 |
| PS           | Physical sciences (subscore)                                  |
| WS           | Writing sample (subcore)                                 |
| BS           | Biological sciences (subscore)                                  |
| MCAT         | Score on the MCAT exam (sum of CR+PS+WS+BS)                                 |
| Apps         | Number of medical schools applied to                                 |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, sex, GPA, PS, WS, and BS to be significant. Looking at the summary we can see that comparing males to females, males have a 2.84 increase in log odds of acceptance versus females. We also found that GPA, PS, and BS all have a negative log odds of admission for each unit of increase. Only WS had a positive log odds of admission for each unit of increase. This model can be used to assess a candidates probability of being accepted into medical school and can be used to give insight into what variables best increase their chance of acceptance. 

```{r}
df.ch3ex2.med <- read.csv("MedGPA.csv")
ch3ex2.med.log <- glm( Accept~.,family=binomial(link='logit'),data=df.ch3ex2.med[,-c(1,3)])
ch3ex2.med.log <- step(ch3ex2.med.log, direction = "backward", trace = 1)
summary(ch3ex2.med.log)
```
Next a decision tree was fit on the same dataset. The tree only found BS to be the variable to split on. The tree does not have as much application in this context due to the limited data set size. If the data set size was larger, the tree would allow a participant to find the best values for each of the predictor variables that would best increase their chance of being accepted into medical school. 

```{r}
ch3ex2.med.tree <- rpart(Accept~., data=df.ch3ex2.med[,-c(1,3)], control = rpart.control(p = 0.0001))
prp(ch3ex2.med.tree, varlen=3)
```

## Bad Health
The second dataset we chose to analyze was the BadHealth dataset from the COUNT package. This dataset contains data about a German health survey data for the year 1998. A table that provides a description of the variables included in the data set is provided below.

| Variable Name           | Description                                                                                 |
|--------------           |---------------------------------------------------------------------------------------------|
| Number of visits        | Number of visits to doctor during 1998                                 |
| bad health              | 1=patient claims to be in bad health; 0=not in bad health                                               |
| age                     | age of patient: 20-60                                                                          |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, number of visits, and age to be significant. Looking at the summary we can see that for every unit increase in the number of visits to the doctor, there is an increase of 0.22 log odds of the patient claiming to be in bad health. For every unit increase in the age of the patient, there is an increase of 0.05 log odds of the patient claiming to be in bad health. This type of model can be used to assess the health of patients using easily accessible data and can be used in policy making.

```{r}
df.ch3ex2.bh <- read.csv("badhealth.csv")
df.ch3ex2.bh$badh <- as.factor(df.ch3ex2.bh$badh)
ch3ex2.med.bh <- glm( badh~.,family=binomial(link='logit'),data=df.ch3ex2.bh[,-c(1)])
ch3ex2.med.bh <- step(ch3ex2.med.bh, direction = "backward", trace = 1)
summary(ch3ex2.med.bh)
```
Next a decision tree was fit on the same dataset. The tree split on number of visits and on age. The tree found that if the number of visits is greater than 14 and the patients age was greater than 36, the patient most likely said they were in bad health. If the number of visits was less than 7.5 or the patient was less than the age of 36, the patient most likely reported they were not in bad health. This type of model can to identify an easy rule of assessing the overall health of a population using the number of visits to the doctor, which would be useful in policy making.

```{r}
ch3ex2.bh.tree <- rpart(badh~., data=df.ch3ex2.bh[,-c(1)], control = rpart.control(p = 0.0001))
prp(ch3ex2.bh.tree, varlen=3)
```

# Chapter 3, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.





