---
title: "INDE498_HW2"
author: "Steven Hwang, 	Haena Kim, Victoria Diaz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)

```
# Chapter 2, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.
  
# Chapter 2, Exercise 4 
Consider the case that, in building linear regression models, there is a concern that some data points may be more important (or more trustable). Thus, it is not uncommon to assign a weight to each data point. Denote the weight for the ith data point as w_i. We still want to estimate the regression parameters in the least squares framework. Follow the process of the derivation of the least squares estimator and propose your new estimator of the regression parameters.

The weighted mean squeare error in matrix is:
\[ min(WMSE(b)) = \frac{1}{n} \sum_{i=1}^{n} w_{i}(y_i - x_ib)^2 \]

We can rewrite $w_i$ in matrix form as $W$, which is a diaganol matrix where the $i_{th}$ diaganol element is the weight for the $x_{i}$ observation. In matrix form this is:
\[  min(WMSE(b)) = \frac{1}{n} (Y-X\beta)^TW(Y-X\beta)\]
Expanding the terms:
\[  min(WMSE(b)) = \frac{1}{n} (Y^TWY-Y^TWX\beta-\beta^TX^TWY+X^T\beta^TWX\beta)\]
Differentiating with respect to $\beta$ and setting equal to zero:
\[  min(WMSE(b)) = \frac{2}{n} (-X^TWY+X^TWX\beta)\]
Setting this equal to 0, we get:
\[  \hat\beta = (X^TWX)^{-1}(X^TWY) \]



# Chapter 3, Exercise 1
Create a new binary variable based on AGE, by labeling the subjects whose age is above the mean of AGE to be class “1” and labeling the subjects whose age is below the mean of AGE to be class “0”. Then, repeat the analysis shown in the R lab of this chapter for the logistic regression model and the analysis shown in the R lab of Chapter 2 for decision tree model. Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

We will use all of the predictors (except for AGE, MMSCORE, TOTAL13, and DX_bl) to predict where a person's age is above or below the mean age. 

## Logistic Regression Model
We begin by loading the data and creating new column named AGE_bin.
AGE_bin: Contains "1" if the subject's age is >= mean(AGE); Contains "0" if the subject's age is < mean(AGE).
```{r}
library(RCurl)
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD.csv"))
AD$ID = c(1:dim(AD)[1])
AD$AGE_bin = ifelse(AD$AGE >= mean(AD$AGE), 1, 0)
AD = AD[ , !(names(AD) %in% c("AGE", "MMSCORE", "TOTAL13", "DX_bl"))]

```

Fitting our model using all of the predictors yields the significant predicators as HippoNV, e4_1, and PTEDUCAT.
```{r}
logit.AD.1 <- glm(AGE_bin ~ ., data = AD[, -c(15)], family = "binomial") 
summary(logit.AD.1)
```

Fitting the model based on the significant predicators of the last model gives us that only two out of the three predictors (HippoNV and e4_1) are actually significant. 
```{r}
logit.AD.2 <- glm(AGE_bin ~ HippoNV + e4_1 + PTEDUCAT , data = AD[, -c(15)], family = "binomial") 
summary(logit.AD.2)
```

We will use the following visualization of the relationships between some of the predicators and the outcome in order to make an educated guess on which other predictors should be considered. 

None of the ploted variables seem to be able to properly classify the data (since all of the pairs of box overlap significantly). 
```{r}
require(reshape2)

AD.long <- melt(AD[,c(2:5, 15, 16)], id.vars = c("ID", "AGE_bin"))

# Plot the data using ggplot
require(ggplot2)
p <- ggplot(AD.long, aes(x = factor(AGE_bin), y = value))

# boxplot, size=.75 to stand out behind CI
p <- p + geom_boxplot(size = 0.75, alpha = 0.5)

# points for observed data
p <- p + geom_point(position = position_jitter(w = 0.05, h = 0), alpha = 0.1)

# diamond at mean for each group
p <- p + stat_summary(fun.y = mean, geom = "point", shape = 18, size = 6,
                      alpha = 0.75, colour = "red")

# confidence limits based on normal distribution
p <- p + stat_summary(fun.data = "mean_cl_normal", geom = "errorbar",
                      width = .2, alpha = 0.8)

p <- p + facet_wrap( ~ variable, scales = "free_y", ncol = 3)


p <- p + labs(title = "Boxplots of variables by mean(age) (0: < mean;1: >= mean)")

print(p)
```

We will use the step() function to automatically choose the best model. The significant variables are  HippoNV, e4_1, and PTEDUCAT. This model explains all but 75.13 of the total deviance with 4 less degrees of freedom. 
```{r} 
logit.AD.full <- glm(AGE_bin ~ ., data = AD[!(names(AD) %in% c("ID"))], family = "binomial")
logit.AD.final <- step(logit.AD.full, direction="both", trace = 0)
summary(logit.AD.final)
```

We can find the 95% confidence intervals of the regression parameters.
```{r}
## CISs of the regression parameters using profiled log-likelihood 
confint(logit.AD.final) 
```

We can also use the Wald Test to test the significance of the regression parameters.
```{r} 
library(aod) 
 
wald.test(b = coef(logit.AD.final), Sigma = vcov(logit.AD.final), Terms = 2)
```

We will now test how our model works on data. We will randomnly choose 200 samples from the AD dataset to make AD.pred. We will create a 95% CI for these predications.
```{r} 
# Dataset that we will test our prediction model on 
AD.pred <- AD[sample(1:dim(AD)[1], 200),]

# pred will have our predictions
pred <- predict(logit.AD.final, AD.pred, type = "link", se.fit = TRUE) 
AD.pred$fit <- pred$fit 
AD.pred$se.fit <- pred$se.fit

# CI for fitted values
AD.pred <- within(AD.pred, {
  # add "fitted" to make predictions at appended temp values 
  fitted = exp(fit)/(1 + exp(fit))
  fit.lower = exp(fit - 1.96 * se.fit) / (1 + exp(fit - 1.96 * se.fit))
  fit.upper = exp(fit + 1.96 *se.fit) / (1 + exp(fit + 1.96 * se.fit))
})


# Visualizing the predication 
library(ggplot2)
newData <- AD.pred[order(AD.pred$AGE_bin),]
p <- ggplot(newData, aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, y = AGE_bin))
# predicted curve and point-wise 95% CI
p <- p + geom_ribbon(aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, ymin = fit.lower, ymax = fit.upper), alpha = 0.2)
p <- p + geom_line(aes(x = PTGENDER + PTEDUCAT + HippoNV + e4_1, y = fitted), colour="red")
# fitted values
p <- p + geom_point(aes(y = fitted), size=2, colour="red")
# observed values
p <- p + geom_point(size = 2)
p <- p + ylab("Probability")
p <- p + labs(title = "Observed and predicted probability of Predicting Age")
print(p)
```

We will test whether or not there is a lack-of-fit. Since dev.p.val is 8.162904e-05, which is not greater than 0.10, there is a large lack of model fit. We conclude that the error in our predictions are coming from a lack of fit from the model. 
```{r}
# Test residual deviance for lack-of-fit (if > 0.10, little-to-no lack-of-fit)
dev.p.val <- 1 - pchisq(logit.AD.final$deviance, logit.AD.final$df.residual)
dev.p.val
```

We will conclude by computing the odds ratios for our predicators and their corresponding 95% confidence intervals to determine the influence of the predictors. 
```{r}
## odds ratios and 95% CI
exp(cbind(OR = coef(logit.AD.final), confint(logit.AD.final)))
```

## Decision Tree
We now create a decision tree based on the dataset. We see that the splitting happens with regards to the predictors HippoNV, FDG, rs3865444, and rs610932. 
```{r}
AD$AGE_bin <- as.factor(AD$AGE_bin)
AD.tree <- rpart(AGE_bin ~., data = AD[!(names(AD) %in% c("ID"))])
prp(AD.tree, varlen=5)
```

When we look at the variable importance of each predictor, we see that the most important variables are HippoNV and FDG. 
```{r}
print(AD.tree$variable.importance)
```


Our objective is now to prune the tree. Testing different different values for cp, we see that our decision tree is most accurate when our tree has about 3 to 4 leaves. 
```{r,cache=FALSE}
library(magrittr)
library(tidyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(dplyr)
library(partykit)

set.seed(1)
train.ix <- sample(nrow(AD),floor( nrow(AD)/2) )
err.train.v <- NULL
err.test.v <- NULL
leaf.v <- NULL
for(i in seq(0.2,0,by=-0.005) ){
  tree <- rpart( AGE_bin ~ ., data = AD[train.ix,], cp=i  ) 
  pred.train <- predict(tree, AD[train.ix,],type="class")
  pred.test <- predict(tree, AD[-train.ix,],type="class")
  current.err.train <- length(which(pred.train != AD[train.ix,]$AGE_bin))/length(pred.train)
  current.err.test <- length(which(pred.test != AD[-train.ix,]$AGE_bin))/length(pred.test)
  err.train.v <- c(err.train.v, current.err.train)
  err.test.v <- c(err.test.v, current.err.test)
  leaf.v <- c(leaf.v, length(which(tree$frame$var == "<leaf>")))
}
err.mat <- as.data.frame( cbind( train_err = err.train.v, test_err = err.test.v , leaf_num = leaf.v ) )
err.mat$leaf_num <- as.factor( err.mat$leaf_num  )
err.mat <- unique(err.mat)
err.mat <- err.mat %>% gather(type, error, train_err,test_err)

# visualizing this 
data.plot <- err.mat %>% mutate(type = factor(type))
ggplot(data.plot, aes(x=leaf_num, y=error, shape = type, color=type)) + geom_line() +
  geom_point(size=3) 
```

We plot a more optimal decision tree which is only dependent on the two most important variables. 
```{r}
tree_0.05 <- prune(tree,cp =0.0319, depth = 3)
prp(tree_0.05,nn.cex=1)
```

Both the logistic regression and the decision tree showed the importance of HippoNV as a predictor. The logistic model showed that PTGENDER, PTEDUCAT, and e4_1 are important predictors, while the decision tree showed that FDG is an important predictor. 


# Chapter 3, Exercise 2
Find two datasets from the UCI data repository or R datasets. Conduct a detailed analysis for both datasets using both logistic regression model and the tree model, e.g., for regression model, you may want to conduct model selection, model comparison, testing of the significance of the regression parameters, evaluation of the R-squared and significance of the model. Also comment on the application of your model on the context of the dataset you have selected.

## Medical School Admission
The first dataset we chose to analyze was the MedGPA dataset from the Stat2Data package. This dataset contains data about medical school admission status and information on GPA and standardized test scores. A table that provides a description of the variables included in the data set is provided below.

| Variable Name| Description                                                                                 |
|--------------|---------------------------------------------------------------------------------------------|
| Accept       | Status: A=accepted to medical school or D=denied admission                                  |
| Acceptance   | Indicator for Accept: 1=accepted or 0=denied                                                |
| Sex          | F=female or M=male                                                                          |
| BCPM         | fuel consumption miles per US gallon                                                        |
| GPA          | College grade point average                                 |
| VR           | Verbal reasoning (subscore                                 |
| PS           | Physical sciences (subscore)                                  |
| WS           | Writing sample (subcore)                                 |
| BS           | Biological sciences (subscore)                                  |
| MCAT         | Score on the MCAT exam (sum of CR+PS+WS+BS)                                 |
| Apps         | Number of medical schools applied to                                 |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, sex, GPA, PS, WS, and BS to be significant. Looking at the summary we can see that comparing males to females, males have a 2.84 increase in log odds of acceptance versus females. We also found that GPA, PS, and BS all have a negative log odds of admission for each unit of increase. Only WS had a positive log odds of admission for each unit of increase. This model can be used to assess a candidates probability of being accepted into medical school and can be used to give insight into what variables best increase their chance of acceptance. 

```{r}
df.ch3ex2.med <- read.csv("MedGPA.csv")
ch3ex2.med.log <- glm( Accept~.,family=binomial(link='logit'),data=df.ch3ex2.med[,-c(1,3)])
ch3ex2.med.log <- step(ch3ex2.med.log, direction = "backward", trace = 1)
summary(ch3ex2.med.log)
```
Next a decision tree was fit on the same dataset. The tree only found BS to be the variable to split on. The tree does not have as much application in this context due to the limited data set size. If the data set size was larger, the tree would allow a participant to find the best values for each of the predictor variables that would best increase their chance of being accepted into medical school. 

```{r}
ch3ex2.med.tree <- rpart(Accept~., data=df.ch3ex2.med[,-c(1,3)], control = rpart.control(p = 0.0001))
prp(ch3ex2.med.tree, varlen=3)
```

## Bad Health
The second dataset we chose to analyze was the BadHealth dataset from the COUNT package. This dataset contains data about a German health survey data for the year 1998. A table that provides a description of the variables included in the data set is provided below.

| Variable Name           | Description                                                                                 |
|--------------           |---------------------------------------------------------------------------------------------|
| Number of visits        | Number of visits to doctor during 1998                                 |
| bad health              | 1=patient claims to be in bad health; 0=not in bad health                                               |
| age                     | age of patient: 20-60                                                                          |

A logistic regression model was fitted using a backwards step variable selection. The final found the intercept, number of visits, and age to be significant. Looking at the summary we can see that for every unit increase in the number of visits to the doctor, there is an increase of 0.22 log odds of the patient claiming to be in bad health. For every unit increase in the age of the patient, there is an increase of 0.05 log odds of the patient claiming to be in bad health. This type of model can be used to assess the health of patients using easily accessible data and can be used in policy making.

```{r}
df.ch3ex2.bh <- read.csv("badhealth.csv")
df.ch3ex2.bh$badh <- as.factor(df.ch3ex2.bh$badh)
ch3ex2.med.bh <- glm( badh~.,family=binomial(link='logit'),data=df.ch3ex2.bh[,-c(1)])
ch3ex2.med.bh <- step(ch3ex2.med.bh, direction = "backward", trace = 1)
summary(ch3ex2.med.bh)
```
Next a decision tree was fit on the same dataset. The tree split on number of visits and on age. The tree found that if the number of visits is greater than 14 and the patients age was greater than 36, the patient most likely said they were in bad health. If the number of visits was less than 7.5 or the patient was less than the age of 36, the patient most likely reported they were not in bad health. This type of model can to identify an easy rule of assessing the overall health of a population using the number of visits to the doctor, which would be useful in policy making.

```{r}
ch3ex2.bh.tree <- rpart(badh~., data=df.ch3ex2.bh[,-c(1)], control = rpart.control(p = 0.0001))
prp(ch3ex2.bh.tree, varlen=3)
```

# Chapter 3, Exercise 3
Pick up any dataset you have used, and randomly split the data into two halves. Use one half to build the tree model and the regression model. Test the models’ prediction performances on the second half. Report what you have found, adjust your way of model building, and suggest a strategy to find the model you consider as the best.





