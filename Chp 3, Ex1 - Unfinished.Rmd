# Chapter 3, Exercise 1

Create a new binary variable based on AGE, by labeling the subjects whose age is above the mean of AGE to be class “1” and labeling the subjects whose age is below the mean of AGE to be class “0”. Then, repeat the analysis shown in the R lab of this chapter for the logistic regression model and the analysis shown in the R lab of Chapter 2 for decision tree model. Identify the final models you would select, evaluate the models, and compare the regression model with the tree model.

*********************************************************************************

Questions: Is the question asking us to make a new binary age variable, erase the original age variable and use DX_bl as the output variable? If so, then the results are very similar to the ones that he got in his book. 

Since the results were so similar, I figured that maybe he wanted us to predict the age instead of whether or not the person is diseased (?) 

-- Also, if I hadn't deleted the AGE column, I would have gotten the same results as the book. 

*********************************************************************************

We begin by loading the data, creating an identifier column, and creating new column named AGE_bin.
AGE_bin: Contains "1" if the subject's age is >= mean(AGE); Contains "0" if the subject's age is < mean(AGE).
We will also delete the columns AGE, MMSCORE, TOTAL13, and ID since we will not take those variables into consideration in your anaylsis.

```{r}
library(RCurl)
AD <- read.csv(text=getURL("https://raw.githubusercontent.com/shuailab/ind_498/master/resource/data/AD.csv"))
AD$ID = c(1:dim(AD)[1])
AD$AGE_bin = ifelse(AD$AGE >= mean(AD$AGE), 1, 0)
AD = AD[ , !(names(AD) %in% c("AGE", "MMSCORE", "TOTAL13"))]

```

We will first create a logistic model use to predict the diagnosis of the subject as either diseased or nondiseased while taking our new column (AGE_bin) into consideration. 

We begin by creating a model with only one predictor: AGE_bin. Looking at the model, we see that AGE_bin is a significant predicator, and it only explains 711.27 - 690.17 = 21.1 out of the total deviance. We notice that a lot the model's deviance is unexplained by the predictor. 

```{r}
logit.AD <- glm(DX_bl ~ AGE_bin, data = AD, family = "binomial") 
summary(logit.AD)
```

Instead of randomly choosing predicators to consider, we will use the following visualization of the relationships between the predicators and outcome to in order to make an educated guess on which other predictors should be considered. 

We see that FDG and HippoNV can be good indicators of whether or not a person has Alzheimer's since the boxes are narrow and do not intersect. The plot corresponding to AGE_bin is not that helpful since we cannot see the multiplicity of the points. 

```{r}
require(reshape2)

AD.long <- melt(AD[,c(1,3,4,5,6,16, 17)], id.vars = c("ID", "DX_bl"))

# Plot the data using ggplot
require(ggplot2)
p <- ggplot(AD.long, aes(x = factor(DX_bl), y = value))

# boxplot, size=.75 to stand out behind CI
p <- p + geom_boxplot(size = 0.75, alpha = 0.5)

# points for observed data
p <- p + geom_point(position = position_jitter(w = 0.05, h = 0), alpha = 0.1)

# diamond at mean for each group
p <- p + stat_summary(fun.y = mean, geom = "point", shape = 18, size = 6,
                      alpha = 0.75, colour = "red")

# confidence limits based on normal distribution
p <- p + stat_summary(fun.data = "mean_cl_normal", geom = "errorbar",
                      width = .2, alpha = 0.8)

p <- p + facet_wrap( ~ variable, scales = "free_y", ncol = 3)

p <- p + labs(title = "Boxplots of variables by diagnosis (0 - normal; 1 - patient)")
print(p)
```

We will use the step() function to automatically choose the best model. 

```{r} 
logit.AD.full <- glm(DX_bl ~ ., data = AD[!(names(AD) %in% c("ID"))], family = "binomial")
logit.AD.final <- step(logit.AD.full, direction="both", trace = 0)
summary(logit.AD.final)
```
